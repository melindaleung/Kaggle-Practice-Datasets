# Titanic

*To see this on Kaggle, click [here](https://www.kaggle.com/melindaleung/titanic).

### Background

On April 15, 1912, the RMS Titanic collided with an iceberg, killing 1502 out of 2224 passengers and crew members. This is one of the most infamous shipwrecks in history. Lack of lifeboats for passengers and crew was a major problem, and since then, stricter safety regulations have been implemented.

### Problem Statement

Predict who is likely to survive the Titanic sinking. This is a binary classification task with 12 variables (including 1 target variable) and 1309 observations.

### Performance Metric

Accuracy

### Reflection

This was my first time completing an entire Kaggle competition. Previously, I did most of my work in R and using Python was different. I didn't find the language as intuitive and it also made me really appreciate R's ggplot. However, the model fitting portion was definitely easier with Python.

Ways to Improve:
- Use ensemble modeling: According to Kaggle, ensembling is a form of meta learning that combines two or more classifiers by averaging or voting. I've gone over other notebooks that discuss ensembling methods, but I personally don't think I understand the theoretical concepts well enough to use them. This is definitely a goal for the future.
- Better understand what models to use: I am still learning when is the best time to use what algorithm for different type of problems. My next goal is to understand more mathematical and statisitcal concepts behind these algorithms.


Overall, I am happy with my results. With more practice, I am sure my score will improve.
